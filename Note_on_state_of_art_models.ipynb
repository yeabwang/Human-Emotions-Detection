{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcOPLd1I8QVHTUBfzy2FJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeabwang/Human-Emotions-Detection/blob/main/Note_on_state_of_art_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Sequential API\n",
        "\n",
        "The Sequential API is the simplest way to create a model in TensorFlow using Keras. It allows you to stack layers sequentially, meaning each layer has exactly one input tensor and one output tensor. This is suitable for most feedforward neural networks where the model is a straight line of layers."
      ],
      "metadata": {
        "id": "-QdO5HO_ea7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 3,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "\n",
        "lenet_model = tf.keras.Sequential(\n",
        "    [\n",
        "    InputLayer(shape = (None, None, 3), ),\n",
        "    # Accepts images of any height and width (None, None) with 3 color channels (RGB).\n",
        "    # This makes the model flexible to different image sizes.\n",
        "\n",
        "    resize_rescale_layers,\n",
        "    # Resizes images to a fixed shape (e.g., 32x32 or 64x64) for the model.\n",
        "    # Rescales pixel values (e.g., from [0, 255] to [0, 1]) for normalization.\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"] , kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"] , padding='valid',\n",
        "          activation = 'relu',kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = CONFIGURATION[\"POOL_SIZE\"], strides= CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "    Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"] ),\n",
        "\n",
        "    # Conv2D Layer:\n",
        "    # filters: Number of filters for feature extraction.\n",
        "    # kernel_size: Size of the convolution window (e.g., 3x3).\n",
        "    # strides: Step size for sliding the kernel over the input.\n",
        "    # padding='valid': No padding, reducing output size.\n",
        "    # activation='relu': Introduces non-linearity.\n",
        "    # kernel_regularizer=L2(...): Applies L2 regularization to reduce overfitting.\n",
        "\n",
        "    # BatchNormalization:\n",
        "    # Normalizes the output of the Conv2D layer, speeding up training and providing regularization.\n",
        "    # MaxPool2D:\n",
        "\n",
        "    # pool_size: Size of the pooling window.\n",
        "    # strides: How far the pooling window moves each step.\n",
        "    # Reduces spatial dimensions while preserving key features.\n",
        "\n",
        "    # Dropout:\n",
        "    # rate: Fraction of neurons to drop (e.g., 0.5 drops 50%).\n",
        "    # Prevents overfitting by promoting generalization.\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"]*2 + 4, kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid',\n",
        "          activation = 'relu', kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = CONFIGURATION[\"POOL_SIZE\"], strides= CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "\n",
        "    Flatten(),\n",
        "    # Converts the 2D feature maps into a 1D vector, preparing it for fully connected layers.\n",
        "\n",
        "    Dense( CONFIGURATION[\"N_DENSE_1\"], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "\n",
        "    Dense( CONFIGURATION['N_DENSE_2'], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense Layers:\n",
        "\n",
        "    # N_DENSE_1 and N_DENSE_2: Number of neurons in each fully connected layer.\n",
        "    # activation='relu': Non-linearity for complex pattern learning.\n",
        "    # kernel_regularizer=L2(...): Regularization to reduce overfitting.\n",
        "    # BatchNormalization and Dropout:\n",
        "\n",
        "    # Consistent use of Batch Norm for faster convergence.\n",
        "    # Dropout helps in generalization by randomly turning off neurons.\n",
        "\n",
        "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\"),\n",
        "    # Output Layer:\n",
        "    # NUM_CLASSES: Number of classes for classification (e.g., 10 for CIFAR-10).\n",
        "    # activation='softmax': Converts outputs to probabilities for each class.\n",
        "\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "Hjx7leiPfsvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Functional API\n",
        "\n",
        "Unlike the Sequential API, where layers are stacked one after another, the Functional API lets you define the computation graph as a directed acyclic graph (DAG).\n",
        "You explicitly define the flow of data between layers, making it suitable for complex neural network architectures.\n",
        "\n",
        "###When to Use the Functional API:\n",
        "When you need multiple inputs or outputs.\n",
        "When you want to use shared layers.\n",
        "When building architectures with skip connections or residual blocks.\n",
        "When you need more flexibility and control over the model design."
      ],
      "metadata": {
        "id": "4QxPcF2ThprY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.regularizers import L2\n",
        "\n",
        "# Configuration Dictionary\n",
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 3,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "\n",
        "# Resize and Rescale Layer\n",
        "# - Resizes input images to (IM_SIZE, IM_SIZE)\n",
        "# - Rescales pixel values from [0, 255] to [0, 1]\n",
        "resize_rescale_layer = tf.keras.Sequential([\n",
        "    layers.Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# ========================\n",
        "#    Feature Extractor\n",
        "# ========================\n",
        "\n",
        "# Input Layer\n",
        "# - Accepts images of any height and width with 3 color channels\n",
        "future_input = Input(shape=(None, None, 3))\n",
        "\n",
        "# Resize and Rescale Layer\n",
        "# - Ensures all images are of the same size and normalized\n",
        "x = resize_rescale_layer(future_input)\n",
        "\n",
        "# Convolutional Layer 1\n",
        "# - Extracts low-level features like edges and textures\n",
        "# - Uses L2 regularization to prevent overfitting\n",
        "x = Conv2D(filters=CONFIGURATION[\"N_FILTERS\"],\n",
        "           kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides=CONFIGURATION[\"N_STRIDES\"],\n",
        "           padding='valid',\n",
        "           activation='relu',\n",
        "           kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "\n",
        "# Batch Normalization\n",
        "# - Normalizes the output of Conv layer\n",
        "# - Accelerates training and provides regularization\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Max Pooling Layer\n",
        "# - Reduces spatial dimensions\n",
        "# - Keeps only the most prominent features\n",
        "x = MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"],\n",
        "                 strides=CONFIGURATION[\"N_STRIDES\"]*2)(x)\n",
        "\n",
        "# Convolutional Layer 2\n",
        "# - Extracts more complex patterns by increasing filters\n",
        "x = Conv2D(filters=CONFIGURATION[\"N_FILTERS\"]*2 + 4,\n",
        "           kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides=CONFIGURATION[\"N_STRIDES\"],\n",
        "           padding='valid',\n",
        "           activation='relu',\n",
        "           kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "\n",
        "# Batch Normalization\n",
        "# - Normalizes output of the second Conv layer\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Max Pooling Layer\n",
        "# - Further reduces spatial dimensions\n",
        "x = MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"],\n",
        "                 strides=CONFIGURATION[\"N_STRIDES\"]*2)(x)\n",
        "\n",
        "# Flatten Layer\n",
        "# - Converts 2D feature maps to 1D feature vector\n",
        "future_output = Flatten()(x)\n",
        "\n",
        "# Create Feature Extractor Model\n",
        "future_model = Model(inputs=future_input, outputs=future_output, name=\"Feature_Extractor\")\n",
        "\n",
        "# ========================\n",
        "#   Classification Model\n",
        "# ========================\n",
        "\n",
        "# Input Layer for Classification Model\n",
        "class_input = Input(shape=(None, None, 3))\n",
        "\n",
        "# Use Feature Extractor\n",
        "# - Reuses the feature extraction part of the model\n",
        "x = future_model(class_input)\n",
        "\n",
        "# Fully Connected Layer 1\n",
        "# - Learns complex patterns and combinations of features\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_1\"],\n",
        "          activation='relu',\n",
        "          kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "\n",
        "# Batch Normalization\n",
        "# - Speeds up training and stabilizes learning\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Dropout Layer\n",
        "# - Randomly sets a fraction of inputs to 0\n",
        "# - Prevents overfitting by adding noise\n",
        "x = Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"])(x)\n",
        "\n",
        "# Output Layer\n",
        "# - Dense layer with softmax activation for multi-class classification\n",
        "class_output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax')(x)\n",
        "\n",
        "# Create Classification Model\n",
        "class_model = Model(inputs=class_input, outputs=class_output, name=\"Classification_Model\")\n",
        "\n",
        "# Model Summary\n",
        "class_model.summary()"
      ],
      "metadata": {
        "id": "nUVQ9lYdlHH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction to Model Subclassing in TensorFlow\n",
        "\n",
        "Model subclassing in TensorFlow is a flexible way to build neural networks by directly inheriting from tf.keras.Model. This approach allows you to fully customize the forward pass (call() method) and define complex architectures that aren't easily implemented using the Sequential or Functional APIs.\n",
        "\n"
      ],
      "metadata": {
        "id": "I3zEvaN6eNPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 3,\n",
        "    \"DROPOUT_RATE\": 0.2,\n",
        "    \"REGULARIZATION_RATE\": 1e-4,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "\n",
        "# Preprocessing Layer: Resizes images and rescales pixel values to [0, 1]\n",
        "resize_rescale_layer = tf.keras.Sequential([\n",
        "    layers.Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# Feature Extractor Class\n",
        "class LeNetModelFutureExtractor(Layer):\n",
        "    def __init__(self, config):\n",
        "        super(LeNetModelFutureExtractor, self).__init__()\n",
        "\n",
        "        # Store configuration for reuse\n",
        "        self.config = config\n",
        "\n",
        "        # Preprocessing Layer\n",
        "        self.resize_rescale = resize_rescale_layer\n",
        "\n",
        "        # First Convolutional Block\n",
        "        self.conv1 = layers.Conv2D(\n",
        "            filters=config[\"N_FILTERS\"],           # Number of filters\n",
        "            kernel_size=config[\"KERNEL_SIZE\"],     # Size of the convolution kernel\n",
        "            strides=config[\"N_STRIDES\"],           # Stride length\n",
        "            padding='valid',                       # No padding, reduces output size\n",
        "            activation='relu',                     # Activation function for non-linearity\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"]) # L2 regularization\n",
        "        )\n",
        "        self.bn1 = layers.BatchNormalization()    # Normalizes activations to stabilize learning\n",
        "        self.pool1 = layers.MaxPooling2D(\n",
        "            pool_size=config[\"POOL_SIZE\"],          # Downsamples feature map size\n",
        "            strides=config[\"N_STRIDES\"]*2           # Stride for pooling\n",
        "        )\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        self.conv2 = layers.Conv2D(\n",
        "            filters=config[\"N_FILTERS\"]*2 + 4,     # Increasing number of filters\n",
        "            kernel_size=config[\"KERNEL_SIZE\"],\n",
        "            strides=config[\"N_STRIDES\"],\n",
        "            padding='valid',\n",
        "            activation='relu',\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"])\n",
        "        )\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.pool2 = layers.MaxPooling2D(\n",
        "            pool_size=config[\"POOL_SIZE\"],\n",
        "            strides=config[\"N_STRIDES\"]*2\n",
        "        )\n",
        "\n",
        "        # Flatten Layer\n",
        "        self.flatten = layers.Flatten()            # Converts 2D feature maps to 1D vector\n",
        "\n",
        "    def call(self, x):\n",
        "        # Forward pass through the feature extractor\n",
        "        x = self.resize_rescale(x)                 # Resize and Rescale Input\n",
        "        x = self.conv1(x)                          # First Convolutional Layer\n",
        "        x = self.bn1(x)                            # Batch Normalization\n",
        "        x = self.pool1(x)                          # Max Pooling\n",
        "        x = self.conv2(x)                          # Second Convolutional Layer\n",
        "        x = self.bn2(x)                            # Batch Normalization\n",
        "        x = self.pool2(x)                          # Max Pooling\n",
        "        x = self.flatten(x)                        # Flatten for Dense Layers\n",
        "        return x\n",
        "\n",
        "# Classification Model Class\n",
        "class LeNetClassification(Model):\n",
        "    def __init__(self, config):\n",
        "        super(LeNetClassification, self).__init__()\n",
        "\n",
        "        # Feature Extractor (Reusable Component)\n",
        "        self.feature_extractor = LeNetModelFutureExtractor(config)\n",
        "\n",
        "        # Fully Connected Layer 1\n",
        "        self.fc1 = layers.Dense(\n",
        "            config[\"N_DENSE_1\"],                   # Number of neurons in the layer\n",
        "            activation='relu',                     # Activation function\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"]) # L2 regularization\n",
        "        )\n",
        "        self.bn3 = layers.BatchNormalization()     # Normalizes activations\n",
        "        self.dropout1 = layers.Dropout(             # Dropout for regularization\n",
        "            rate=config[\"DROPOUT_RATE\"]\n",
        "        )\n",
        "\n",
        "        # Fully Connected Layer 2\n",
        "        self.fc2 = layers.Dense(\n",
        "            config[\"N_DENSE_2\"],\n",
        "            activation='relu',\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"])\n",
        "        )\n",
        "        self.bn4 = layers.BatchNormalization()\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = layers.Dense(\n",
        "            config[\"NUM_CLASSES\"],                 # Number of output classes\n",
        "            activation='softmax'                   # Softmax for multi-class classification\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Forward pass through the classification model\n",
        "        x = self.feature_extractor(x)              # Extract Features\n",
        "        x = self.fc1(x)                            # Fully Connected Layer 1\n",
        "        x = self.bn3(x)                            # Batch Normalization\n",
        "        x = self.dropout1(x)                       # Dropout\n",
        "        x = self.fc2(x)                            # Fully Connected Layer 2\n",
        "        x = self.bn4(x)                            # Batch Normalization\n",
        "        x = self.output_layer(x)                   # Output Layer\n",
        "        return x\n",
        "\n",
        "# Instantiate the Classification Model\n",
        "leNet_class_model = LeNetClassification(CONFIGURATION)\n",
        "\n",
        "# Build Model with Input Shape to initialize weights\n",
        "leNet_class_model.build((None, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3))\n",
        "\n",
        "# Display Model Summary\n",
        "leNet_class_model.summary()"
      ],
      "metadata": {
        "id": "vJQFimXNrQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frrl2MIc0Zag"
      },
      "outputs": [],
      "source": [
        "#AlexNet was a breakthrough in deep learning, winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 and pioneering modern CNN architectures.\n",
        "\n",
        "# Architecture: 8 layers (5 convolutional + 3 fully connected).\n",
        "\n",
        "# Input Size: 227 × 227 × 3 (RGB images).\n",
        "\n",
        "# Convolutional Layers:\n",
        "#### Conv1: 96 filters, 11×11 kernel, stride 4, ReLU.\n",
        "#### Conv2: 256 filters, 5×5 kernel, stride 1, ReLU.\n",
        "#### Conv3: 384 filters, 3×3 kernel, stride 1, ReLU.\n",
        "#### Conv4: 384 filters, 3×3 kernel, stride 1, ReLU.\n",
        "#### Conv5: 256 filters, 3×3 kernel, stride 1, ReLU.\n",
        "\n",
        "# Max Pooling: After Conv1, Conv2, and Conv5 (3×3 kernel, stride 2).\n",
        "\n",
        "# Fully Connected Layers:\n",
        "#### FC6: 4096 neurons, ReLU.\n",
        "#### FC7: 4096 neurons, ReLU.\n",
        "#### FC8 (Output): 1000 neurons (ImageNet classes), Softmax.\n",
        "\n",
        "# Activation Function: ReLU (introduced to speed up training).\n",
        "# Normalization: Local Response Normalization (LRN) after Conv1 and Conv2.\n",
        "# Regularization: Dropout (0.5) in FC6 and FC7.\n",
        "# Optimization: Stochastic Gradient Descent (SGD) with momentum (0.9).\n",
        "# Batch Size: 128.\n",
        "# Weight Initialization: Gaussian distribution.\n",
        "# Data Augmentation: Cropping, flipping, and color jittering.\n",
        "# Training Dataset: ImageNet (1.2 million images, 1000 classes).\n",
        "# Parallel Training: Two GPUs used to split model layers for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## VGG Model\n",
        "# Key Features:\n",
        "# Deep Network: 16 (VGG16) or 19 (VGG19) layers.\n",
        "# Uniform Kernel Size: Only 3×3 convolution layers to maintain consistency.\n",
        "# Increased Depth: More layers compared to AlexNet for hierarchical feature learning.\n",
        "# Regularization: Dropout (0.5) in fully connected layers.\n",
        "# Optimization: SGD with momentum (0.9), batch size = 256.\n",
        "# Weight Initialization: Pretrained on ImageNet, useful for transfer learning.\n",
        "# Data Augmentation: Cropping, flipping, and color jittering\n",
        "# VGG16 and VGG19 are the most common variants. #the main difference here is the number of convulational neurons used vgg16 used 13 convulational neurons and the vgg 19 uses the 16 convulational neurons\n",
        "# Stacked small convolutional filters (3×3 kernel, stride 1, padding 1) for deeper representations.\n",
        "# Uses 2×2 max pooling (stride 2) after every block for downsampling.\n",
        "\n",
        "\n",
        "# Layers - Vgg16\n",
        "# Input Size: 224 × 224 × 3 (RGB images).\n",
        "\n",
        "# Conv Layers:\n",
        "#### Block 1: 2 × (64 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 2: 2 × (128 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 3: 3 × (256 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 4: 3 × (512 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 5: 3 × (512 filters, 3×3, ReLU) → Max Pooling\n",
        "\n",
        "# Fully Connected Layers:\n",
        "#### FC6: 4096 neurons, ReLU\n",
        "#### FC7: 4096 neurons, ReLU\n",
        "#### FC8 (Output): 1000 neurons (Softmax for classification)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wz7UjWm053uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RESNET MODEL\n",
        "\n",
        "# ResNet introduced residual learning to address the vanishing gradient problem, allowing for extremely deep networks.\n",
        "\n",
        "# Key Features:\n",
        "#### Deep Architecture: Can scale up to ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152.\n",
        "#### Residual Connections (Skip Connections):\n",
        "####### Instead of directly learning H(x), it learns F(x) = H(x) - x, making optimization easier.\n",
        "#### Helps gradients flow smoothly during backpropagation.\n",
        "#### Batch Normalization: Used after every convolution to stabilize training.\n",
        "#### ReLU Activation: Applied after each convolutional layer.\n",
        "\n",
        "# ResNet-18 Layer-by-Layer Breakdown\n",
        "# Here is the detailed layer-wise breakdown for ResNet-18:\n",
        "\n",
        "# Conv1 (Initial Convolutional Layer):\n",
        "# Operation: 7×7 Convolution, 64 filters, stride 2\n",
        "# Output Size: 112 × 112 × 64\n",
        "\n",
        "# MaxPool:\n",
        "# Operation: 3×3 Max Pooling, stride 2\n",
        "# Output Size: 56 × 56 × 64\n",
        "\n",
        "# Conv2_x (Residual Block 1 and 2):\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 64 filters)\n",
        "# Output Size: 56 × 56 × 64\n",
        "\n",
        "# Conv3_x (Residual Block 3 and 4):\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 128 filters), stride 2\n",
        "# Output Size: 28 × 28 × 128\n",
        "\n",
        "# Conv4_x (Residual Block 5 and 6):\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 256 filters), stride 2\n",
        "# Output Size: 14 × 14 × 256\n",
        "\n",
        "# Conv5_x (Residual Block 7 and 8):\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 512 filters), stride 2\n",
        "# Output Size: 7 × 7 × 512\n",
        "\n",
        "# AvgPool (Global Average Pooling):\n",
        "# Operation: Global Average Pooling\n",
        "# Output Size: 1 × 1 × 512\n",
        "\n",
        "# Fully Connected (FC):\n",
        "# Operation: Fully Connected layer (512 → 1000 classes)\n",
        "# Output Size: 1 × 1 × 1000 (classification result)\n",
        "\n",
        "\n",
        "## So we can see ResNet as a collection of shallow layers with a condition of skipping layers which their cumulative is zero.\n",
        "## Firstly this will help the model avoid vanishing gradient.\n",
        "## Seconly it performs well since it acts like a collection of various shallow layers which the model choose its path based on the conditions.\n",
        "\n"
      ],
      "metadata": {
        "id": "bgyAriga_yfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomConv2D(Layer):\n",
        "  def __init__(self, n_filters, kernel_size, n_strides, padding='valid'):\n",
        "    super(CustomConv2D, self).__init__(name='custom_conv2d')\n",
        "\n",
        "    # Convolution layer with ReLU activation\n",
        "    self.conv = Conv2D(\n",
        "        filters=n_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        activation='relu',  # Apply ReLU activation after convolution\n",
        "        strides=n_strides,\n",
        "        padding=padding)\n",
        "\n",
        "    # Batch normalization layer\n",
        "    self.batch_norm = BatchNormalization()\n",
        "\n",
        "  def call(self, x, training=True):\n",
        "    # Forward pass through Conv2D layer\n",
        "    x = self.conv(x)\n",
        "\n",
        "    # Forward pass through Batch Normalization\n",
        "    x = self.batch_norm(x, training)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Purpose: Encapsulates a Conv2D layer followed by batch normalization and ReLU activation.\n",
        "# Why? In ResNet, the pattern of Conv2D → BatchNorm → ReLU is repeated often, so this class keeps the code DRY (Don't Repeat Yourself).\n",
        "\n",
        "# Line-by-line:\n",
        "# self.conv: Defines a convolution layer with ReLU activation.\n",
        "# self.batch_norm: Adds batch normalization to stabilize learning.\n",
        "# call() method: Implements the forward pass.\n",
        "\n",
        "class ResidualBlock(Layer):\n",
        "  def __init__(self, n_channels, n_strides=1):\n",
        "    super(ResidualBlock, self).__init__(name='res_block')\n",
        "\n",
        "    # Determine if a shortcut is needed\n",
        "    self.dotted = (n_strides != 1)\n",
        "\n",
        "    # First Conv layer with strides (for downsampling)\n",
        "    self.custom_conv_1 = CustomConv2D(n_channels, 3, n_strides, padding=\"same\")\n",
        "    # Second Conv layer with stride of 1\n",
        "    self.custom_conv_2 = CustomConv2D(n_channels, 3, 1, padding=\"same\")\n",
        "\n",
        "    # ReLU Activation for the output\n",
        "    self.activation = Activation('relu')\n",
        "\n",
        "    # If downsampling is required, add a 1x1 convolution\n",
        "    if self.dotted:\n",
        "      self.custom_conv_3 = CustomConv2D(n_channels, 1, n_strides)\n",
        "\n",
        "  def call(self, input, training):\n",
        "    # Forward pass through two convolutional layers\n",
        "    x = self.custom_conv_1(input, training)\n",
        "    x = self.custom_conv_2(x, training)\n",
        "\n",
        "    # Apply skip connection\n",
        "    if self.dotted:\n",
        "      # Projection shortcut using 1x1 Conv if dimensions differ\n",
        "      x_add = self.custom_conv_3(input, training)\n",
        "      x_add = Add()([x, x_add])\n",
        "    else:\n",
        "      # Identity shortcut if dimensions are the same\n",
        "      x_add = Add()([x, input])\n",
        "\n",
        "    # ReLU Activation after addition\n",
        "    return self.activation(x_add)\n",
        "\n",
        "# Purpose: This block learns residual mappings. It adds the input to the output of the convolutional layers (skip connection).\n",
        "# Why? Skip connections allow the network to learn identity mappings easily, which helps with vanishing gradients and deeper networks.\n",
        "\n",
        "# Line-by-line:\n",
        "# self.dotted: Checks if downsampling is needed (i.e., stride > 1).\n",
        "# custom_conv_1 and custom_conv_2: Standard convolution layers.\n",
        "# custom_conv_3: Used for 1x1 projection when downsampling.\n",
        "# Add(): Adds the shortcut connection to the output.\n",
        "# self.activation: ReLU applied after the addition.\n",
        "\n",
        "class ResNet34(Model):\n",
        "  def __init__(self):\n",
        "    super(ResNet34, self).__init__(name='resnet_34')\n",
        "\n",
        "    # Initial Conv Layer and Max Pooling\n",
        "    self.conv_1 = CustomConv2D(64, 7, 2, padding='same')\n",
        "    self.max_pool = MaxPooling2D(3, 2)\n",
        "\n",
        "    # Conv2_x: 3 Residual Blocks\n",
        "    self.conv_2_1 = ResidualBlock(64)\n",
        "    self.conv_2_2 = ResidualBlock(64)\n",
        "    self.conv_2_3 = ResidualBlock(64)\n",
        "\n",
        "    # Conv3_x: 4 Residual Blocks (First Block with Stride 2 for Downsampling)\n",
        "    self.conv_3_1 = ResidualBlock(128, 2)\n",
        "    self.conv_3_2 = ResidualBlock(128)\n",
        "    self.conv_3_3 = ResidualBlock(128)\n",
        "    self.conv_3_4 = ResidualBlock(128)\n",
        "\n",
        "    # Conv4_x: 6 Residual Blocks (First Block with Stride 2 for Downsampling)\n",
        "    self.conv_4_1 = ResidualBlock(256, 2)\n",
        "    self.conv_4_2 = ResidualBlock(256)\n",
        "    self.conv_4_3 = ResidualBlock(256)\n",
        "    self.conv_4_4 = ResidualBlock(256)\n",
        "    self.conv_4_5 = ResidualBlock(256)\n",
        "    self.conv_4_6 = ResidualBlock(256)\n",
        "\n",
        "    # Conv5_x: 3 Residual Blocks (First Block with Stride 2 for Downsampling)\n",
        "    self.conv_5_1 = ResidualBlock(512, 2)\n",
        "    self.conv_5_2 = ResidualBlock(512)\n",
        "    self.conv_5_3 = ResidualBlock(512)\n",
        "\n",
        "    # Global Average Pooling and Fully Connected Layer\n",
        "    self.global_pool = GlobalAveragePooling2D()\n",
        "    self.fc_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax')\n",
        "\n",
        "  def call(self, x, training=True):\n",
        "    # Initial Conv and Pooling\n",
        "    x = self.conv_1(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    # Conv2_x\n",
        "    x = self.conv_2_1(x, training)\n",
        "    x = self.conv_2_2(x, training)\n",
        "    x = self.conv_2_3(x, training)\n",
        "\n",
        "    # Conv3_x\n",
        "    x = self.conv_3_1(x, training)\n",
        "    x = self.conv_3_2(x, training)\n",
        "    x = self.conv_3_3(x, training)\n",
        "    x = self.conv_3_4(x, training)\n",
        "\n",
        "    # Conv4_x\n",
        "    x = self.conv_4_1(x, training)\n",
        "    x = self.conv_4_2(x, training)\n",
        "    x = self.conv_4_3(x, training)\n",
        "    x = self.conv_4_4(x, training)\n",
        "    x = self.conv_4_5(x, training)\n",
        "    x = self.conv_4_6(x, training)\n",
        "\n",
        "    # Conv5_x\n",
        "    x = self.conv_5_1(x, training)\n",
        "    x = self.conv_5_2(x, training)\n",
        "    x = self.conv_5_3(x, training)\n",
        "\n",
        "    # Global Average Pooling and Output Layer\n",
        "    x = self.global_pool(x)\n",
        "    return self.fc_3(x)\n",
        "\n",
        "# Purpose: This is the complete ResNet-34 architecture with 34 layers using custom Residual Blocks.\n",
        "# Why? It closely follows the original ResNet-34 design with grouped residual blocks.\n",
        "\n",
        "# Line-by-line:\n",
        "# Conv2_x to Conv5_x: Groups of residual blocks with downsampling at the start of each stage.\n",
        "# GlobalAveragePooling2D: Reduces each feature map to a single value, preventing overfitting.\n",
        "# Dense Layer: Output layer with softmax activation for classification."
      ],
      "metadata": {
        "id": "oaUp4jsmUfqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Covariate Shift and Batch Normalization\n",
        "\n",
        "# # Covariate Shift\n",
        "# # Covariate Shift refers to a situation where the distribution of the input data changes between training and testing phases, but the conditional distribution of the output given the input remains the same. In simpler terms, it happens when the model is trained on data from one distribution, but when deployed, it encounters data from a different distribution, which can hurt model performance.\n",
        "\n",
        "# # Batch Normalization (BatchNorm)\n",
        "# # Batch Normalization is a technique introduced to address internal covariate shift during the training of deep neural networks. It normalizes the activations of each layer by scaling and shifting them, ensuring that the distribution of inputs to each layer remains stable throughout training.\n",
        "\n",
        "# In 2D Global Average Pooling, the pooling operation averages over all spatial dimensions (height and width) for each feature map (channel) of the input.\n",
        "# Instead of using traditional pooling methods like max pooling (which extracts the maximum value), global average pooling computes the average value of each feature map over its entire spatial area.\n",
        "\n",
        "# In tasks where the position of the pixels in our datasample don't matter we are good to use the global avg pooling, but if the position matters its not recommended to use that."
      ],
      "metadata": {
        "id": "8bw9XadsTgwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # MobileNet\n",
        "\n",
        "# MobileNetV2 is a lightweight deep learning model designed specifically for mobile and embedded devices. It uses depthwise separable convolutions to reduce computation while maintaining accuracy. The key feature of MobileNetV2 is its inverted residual structure, which helps in capturing features with fewer parameters.\n",
        "\n",
        "class DepthwiseSeparableConv(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same', expansion=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "\n",
        "        # Depthwise convolution: applies a single filter per input channel\n",
        "        self.depthwise = layers.DepthwiseConv2D(kernel_size=kernel_size, strides=strides, padding=padding)\n",
        "\n",
        "        # Pointwise convolution: applies 1x1 convolution to combine features\n",
        "        self.pointwise = layers.Conv2D(filters, kernel_size=1, padding='same')\n",
        "\n",
        "        # Batch Normalization to stabilize training and improve generalization\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "        # ReLU activation to introduce non-linearity\n",
        "        self.activation = layers.ReLU()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.depthwise(inputs)  # Apply depthwise convolution\n",
        "        x = self.pointwise(x)       # Apply pointwise convolution\n",
        "        x = self.batch_norm(x, training=training)  # Apply batch normalization\n",
        "        return self.activation(x)  # Apply ReLU activation\n",
        "\n",
        "# Inverted Residual Block (with Linear Bottleneck)\n",
        "class InvertedResidualBlock(layers.Layer):\n",
        "    def __init__(self, input_channels, output_channels, strides=1, expansion=6):\n",
        "        super(InvertedResidualBlock, self).__init__()\n",
        "\n",
        "        # Expansion layer: increases the number of channels (width of the layer)\n",
        "        self.expand = layers.Conv2D(input_channels * expansion, kernel_size=1, padding='same')\n",
        "        self.expand_bn = layers.BatchNormalization()\n",
        "        self.expand_relu = layers.ReLU()\n",
        "\n",
        "        # Depthwise Separable Convolution\n",
        "        self.depthwise = DepthwiseSeparableConv(input_channels * expansion, kernel_size=3, strides=strides, padding='same')\n",
        "\n",
        "        # Projection layer: reduces the number of channels back to the desired output size\n",
        "        self.project = layers.Conv2D(output_channels, kernel_size=1, padding='same')\n",
        "        self.project_bn = layers.BatchNormalization()\n",
        "\n",
        "        # If the input and output channels are the same, add a residual shortcut connection\n",
        "        self.shortcut = None\n",
        "        if strides == 1 and input_channels == output_channels:\n",
        "            self.shortcut = layers.Add()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Expansion phase\n",
        "        x = self.expand(inputs)\n",
        "        x = self.expand_bn(x, training=training)\n",
        "        x = self.expand_relu(x)\n",
        "\n",
        "        # Depthwise separable convolution phase\n",
        "        x = self.depthwise(x, training=training)\n",
        "\n",
        "        # Projection phase\n",
        "        x = self.project(x)\n",
        "        x = self.project_bn(x, training=training)\n",
        "\n",
        "        # If a shortcut is present, add it (residual connection)\n",
        "        if self.shortcut:\n",
        "            return self.shortcut([x, inputs])\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# MobileNetV2 Model\n",
        "class MobileNetV2(Model):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.conv_1 = layers.Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu')\n",
        "        self.bn_1 = layers.BatchNormalization()\n",
        "\n",
        "        # Create inverted residual blocks with different expansion factors and channel sizes\n",
        "        self.block_1 = InvertedResidualBlock(32, 16, strides=1)\n",
        "        self.block_2 = InvertedResidualBlock(16, 24, strides=2)\n",
        "        self.block_3 = InvertedResidualBlock(24, 32, strides=1)\n",
        "        self.block_4 = InvertedResidualBlock(32, 64, strides=2)\n",
        "        self.block_5 = InvertedResidualBlock(64, 96, strides=1)\n",
        "        self.block_6 = InvertedResidualBlock(96, 160, strides=2)\n",
        "        self.block_7 = InvertedResidualBlock(160, 320, strides=1)\n",
        "\n",
        "        # Final convolution to expand the feature maps\n",
        "        self.conv_2 = layers.Conv2D(1280, kernel_size=1, strides=1, padding='same', activation='relu')\n",
        "\n",
        "        # Global Average Pooling to reduce the spatial dimensions to 1x1\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "\n",
        "        # Fully connected layer for classification (softmax for multi-class classification)\n",
        "        self.fc = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Apply the initial convolution and batch normalization\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.bn_1(x, training=training)\n",
        "\n",
        "        # Apply each Inverted Residual Block sequentially\n",
        "        x = self.block_1(x, training=training)\n",
        "        x = self.block_2(x, training=training)\n",
        "        x = self.block_3(x, training=training)\n",
        "        x = self.block_4(x, training=training)\n",
        "        x = self.block_5(x, training=training)\n",
        "        x = self.block_6(x, training=training)\n",
        "        x = self.block_7(x, training=training)\n",
        "\n",
        "        # Final convolution to expand features before pooling\n",
        "        x = self.conv_2(x)\n",
        "\n",
        "        # Global Average Pooling to flatten the feature map into a single vector\n",
        "        x = self.global_pool(x)\n",
        "\n",
        "        # Apply the fully connected layer for classification\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uiVCYjrVZRDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet V3\n",
        "\n",
        "The Squeeze-and-Excitation (SE) Block is a powerful mechanism introduced to enhance the representational power of convolutional neural networks by modeling the interdependencies between channels. It performs channel-wise attention, allowing the network to focus on the most informative features.\n",
        "\n",
        "The reason why this was introduced is the traditional convulational networks use filters which extract features independetly in each channels making it not aware about the knowledge and the relationship with the other channel.\n",
        "\n",
        "So this block will create a relation channel which will help the network to\n",
        "\n",
        "Emphasize informative channels and suppress less useful ones.\n",
        "Enhance feature maps by learning channel-wise importance.\n",
        "Improve model performance with minimal additional computational cost.\n",
        "\n",
        "The SE block has three main stages:\n",
        "\n",
        "1. Squeeze: Global spatial information is aggregated into a single channel descriptor by using Global Average Pooling. This helps the network understand the global context of the feature maps.\n",
        "\n",
        "-- The global avg pooling converts each channel into a single scalar value by averaging the spatial dimensions.\n",
        "\n",
        "2. Excitation: Fully connected layers capture channel-wise dependencies, producing weights for each channel. These weights are then multiplied with the original feature maps to emphasize important channels.\n",
        "\n",
        "-- It contains two fully connected layers:\n",
        "\n",
        "--- First Layer: Reduces the number of channels by a factor of reduction (typically 4), capturing cross-channel dependencies.\n",
        "\n",
        "--- Second Layer: Restores the original number of channels.\n",
        "\n",
        "Both layers use ReLU for introducing Non-Linearity and Sigmoid normalizing the channel weights to the value of 0 and 1.\n",
        "\n",
        "3. Recalibration (Scale and Excite):\n",
        "\n",
        "The weights are multiplied by the original feature maps, recalibrating each channel's importance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yUXG9jr9RlU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient Net\n",
        "\n",
        "EfficientNet is a family of convolutional neural networks that scales up models efficiently in three dimensions:\n",
        "\n",
        "Depth (more layers),\n",
        "Width (wider layers),\n",
        "Resolution (higher input image size)\n",
        "\n",
        "It uses Compound Scaling to uniformly scale all three dimensions, balancing accuracy and computational efficiency.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "*   Compound Scaling: A balanced approach to scaling network depth, width, and resolution.\n",
        "* MBConv Blocks: Inverted residual blocks with squeeze-and-excitation layers, optimized for mobile and edge devices.\n",
        "* Swish Activation: A smoother activation function (x * sigmoid(x)) leading to better performance.\n",
        "* Depthwise Separable Convolutions: Used to reduce computation and parameter count.\n",
        "\n"
      ],
      "metadata": {
        "id": "RRGc8ttBROgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, BatchNormalization, ReLU, GlobalAveragePooling2D, Dense,\n",
        "    DepthwiseConv2D, Add, Layer, Dropout, Activation, Multiply\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Configuration for Learning Rate and Epochs\n",
        "CONFIGURATION = {\n",
        "    \"LEARNING_RATE\": 0.001,\n",
        "    \"N_EPOCHS\": 30\n",
        "}\n",
        "\n",
        "# Swish Activation Function\n",
        "class Swish(Layer):\n",
        "    def call(self, inputs):\n",
        "        return inputs * tf.nn.sigmoid(inputs)\n",
        "\n",
        "# Squeeze and Excitation Block for Channel Attention\n",
        "class SqueezeAndExcite(Layer):\n",
        "    def __init__(self, input_channels, reduction=4):\n",
        "        super(SqueezeAndExcite, self).__init__()\n",
        "        self.global_pool = GlobalAveragePooling2D(keepdims=True)\n",
        "        self.fc1 = Dense(input_channels // reduction, activation='relu')\n",
        "        self.fc2 = Dense(input_channels, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.global_pool(inputs)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return Multiply()([inputs, x])\n",
        "\n",
        "# MBConv Block (Mobile Inverted Bottleneck with Squeeze and Excite)\n",
        "class MBConv(Layer):\n",
        "    def __init__(self, input_channels, output_channels, strides=1, expansion=6, kernel_size=3, se_ratio=0.25):\n",
        "        super(MBConv, self).__init__()\n",
        "        self.strides = strides\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.expanded_channels = input_channels * expansion\n",
        "\n",
        "        # Expansion Phase\n",
        "        self.expand_conv = Conv2D(self.expanded_channels, kernel_size=1, padding='same', use_bias=False)\n",
        "        self.expand_bn = BatchNormalization()\n",
        "        self.expand_swish = Swish()\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        self.depthwise_conv = DepthwiseConv2D(kernel_size=kernel_size, strides=strides, padding='same', use_bias=False)\n",
        "        self.depthwise_bn = BatchNormalization()\n",
        "        self.depthwise_swish = Swish()\n",
        "\n",
        "        # Squeeze and Excite\n",
        "        self.se = SqueezeAndExcite(self.expanded_channels, reduction=int(1/se_ratio))\n",
        "\n",
        "        # Output Projection\n",
        "        self.project_conv = Conv2D(output_channels, kernel_size=1, padding='same', use_bias=False)\n",
        "        self.project_bn = BatchNormalization()\n",
        "\n",
        "        # Shortcut Connection (if input and output shapes match)\n",
        "        self.use_shortcut = (strides == 1 and input_channels == output_channels)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.expand_conv(inputs)\n",
        "        x = self.expand_bn(x, training=training)\n",
        "        x = self.expand_swish(x)\n",
        "\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.depthwise_bn(x, training=training)\n",
        "        x = self.depthwise_swish(x)\n",
        "\n",
        "        x = self.se(x)\n",
        "\n",
        "        x = self.project_conv(x)\n",
        "        x = self.project_bn(x, training=training)\n",
        "\n",
        "        # Add shortcut connection if applicable\n",
        "        if self.use_shortcut:\n",
        "            x = Add()([x, inputs])\n",
        "\n",
        "        return x\n",
        "\n",
        "# EfficientNet Model Implementation\n",
        "class EfficientNet(Model):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(EfficientNet, self).__init__()\n",
        "\n",
        "        # Initial Stem Layer\n",
        "        self.stem_conv = Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False)\n",
        "        self.stem_bn = BatchNormalization()\n",
        "        self.stem_swish = Swish()\n",
        "\n",
        "        # MBConv Blocks (EfficientNetB0 Scaling)\n",
        "        self.block_1 = MBConv(32, 16, strides=1, expansion=1)\n",
        "        self.block_2 = MBConv(16, 24, strides=2, expansion=6)\n",
        "        self.block_3 = MBConv(24, 40, strides=2, expansion=6)\n",
        "        self.block_4 = MBConv(40, 80, strides=2, expansion=6)\n",
        "        self.block_5 = MBConv(80, 112, strides=1, expansion=6)\n",
        "        self.block_6 = MBConv(112, 192, strides=2, expansion=6)\n",
        "        self.block_7 = MBConv(192, 320, strides=1, expansion=6)\n",
        "\n",
        "        # Final Convolution and Pooling\n",
        "        self.conv_head = Conv2D(1280, kernel_size=1, use_bias=False)\n",
        "        self.bn_head = BatchNormalization()\n",
        "        self.head_swish = Swish()\n",
        "        self.global_pool = GlobalAveragePooling2D()\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = Dropout(0.3)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.stem_conv(inputs)\n",
        "        x = self.stem_bn(x, training=training)\n",
        "        x = self.stem_swish(x)\n",
        "\n",
        "        x = self.block_1(x, training=training)\n",
        "        x = self.block_2(x, training=training)\n",
        "        x = self.block_3(x, training=training)\n",
        "        x = self.block_4(x, training=training)\n",
        "        x = self.block_5(x, training=training)\n",
        "        x = self.block_6(x, training=training)\n",
        "        x = self.block_7(x, training=training)\n",
        "\n",
        "        x = self.conv_head(x)\n",
        "        x = self.bn_head(x, training=training)\n",
        "        x = self.head_swish(x)\n",
        "\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize and build the model\n",
        "efficient_net_model = EfficientNet(num_classes=3)\n",
        "efficient_net_model(tf.zeros([1, 256, 256, 3]), training=False)  # Dummy input to build model\n",
        "efficient_net_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "HdONKB6xRLx3",
        "outputId": "fc1f26ed-55c4-4a3b-d088-afede8db6f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"efficient_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"efficient_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m864\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ swish (\u001b[38;5;33mSwish\u001b[0m)                        │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv (\u001b[38;5;33mMBConv\u001b[0m)                     │ ?                           │           \u001b[38;5;34m2,696\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_1 (\u001b[38;5;33mMBConv\u001b[0m)                   │ ?                           │          \u001b[38;5;34m10,296\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_2 (\u001b[38;5;33mMBConv\u001b[0m)                   │ ?                           │          \u001b[38;5;34m22,372\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_3 (\u001b[38;5;33mMBConv\u001b[0m)                   │ ?                           │          \u001b[38;5;34m62,300\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_4 (\u001b[38;5;33mMBConv\u001b[0m)                   │ ?                           │         \u001b[38;5;34m216,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_5 (\u001b[38;5;33mMBConv\u001b[0m)                   │ ?                           │         \u001b[38;5;34m443,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_6 (\u001b[38;5;33mMBConv\u001b[0m)                   │ ?                           │       \u001b[38;5;34m1,275,680\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m1280\u001b[0m)             │         \u001b[38;5;34m409,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_22               │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m1280\u001b[0m)             │           \u001b[38;5;34m5,120\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ swish_15 (\u001b[38;5;33mSwish\u001b[0m)                     │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_7           │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                      │           \u001b[38;5;34m3,843\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ swish (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Swish</span>)                        │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                     │ ?                           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,696</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                   │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,296</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                   │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">22,372</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                   │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">62,300</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                   │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">216,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                   │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">443,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mb_conv_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MBConv</span>)                   │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275,680</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">409,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_22               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ swish_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Swish</span>)                     │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_7           │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,452,579\u001b[0m (9.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,452,579</span> (9.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,437,123\u001b[0m (9.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,437,123</span> (9.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m15,456\u001b[0m (60.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,456</span> (60.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning\n",
        "\n",
        "Transfer learning is a deep learning technique where a pre-trained model is used as a starting point for a different but related task. Instead of training a model from scratch, you \"transfer\" the learned features from a model trained on a large dataset (like ImageNet) to your specific problem.\n",
        "\n",
        "There are two main approaches:\n",
        "\n",
        "* Feature Extraction: Freeze the pre-trained layers and only train the new classifier on top.\n",
        "* Fine-Tuning: Unfreeze some top layers of the base model and train them alongside the new classifier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aKzx-ubiDCsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MobileNetV2 without the top classification layer\n",
        "\n",
        "backbone_model = tf.keras.applications.MobileNetV2(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)\n",
        ")\n",
        "\n",
        "# Freeze the base model layers initially\n",
        "\n",
        "backbone_model.trainable = False\n",
        "\n",
        "# Define the input\n",
        "pretrained_input = Input(shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3))\n",
        "\n",
        "# Build the classification head\n",
        "x = backbone_model(pretrained_input, training = False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Output layer\n",
        "pretrained_output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")(x)\n",
        "\n",
        "# Assemble the model\n",
        "pretrained_model = Model(inputs=pretrained_input, outputs=pretrained_output, name=\"EfficientNetB4_pretrained\")\n",
        "\n",
        "# Display the model architecture\n",
        "pretrained_model.summary()\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('pretrained.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "callbacks = [early_stopping, reduce_lr, model_checkpoint]\n",
        "\n",
        "# Train only the top layers\n",
        "\n",
        "history = pretrained_model.fit(\n",
        "    mixed_dataset.map(cutmix),\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "\n",
        "# Unfreeze the last 20 layers\n",
        "\n",
        "for layer in backbone_model.layers[-20:]:\n",
        "    backbone_model.trainable = True\n",
        "\n",
        "# Recompile with a smaller learning rate for fine-tuning\n",
        "\n",
        "pretrained_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=CategoricalCrossentropy(),\n",
        "    metrics=[CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\")]\n",
        ")\n",
        "\n",
        "# Continue training with fine-tuning\n",
        "\n",
        "history_fine = pretrained_model.fit(\n",
        "    mixed_dataset.map(cutmix),\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "AtlQlA9MVtV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature map model\n",
        "\n",
        "Definition: A feature map is the output of a convolutional layer after applying filters (kernels) to the input image or previous layer's output.\n",
        "\n",
        "Purpose: It captures different aspects of the input, such as edges, textures, patterns, and more complex features at deeper layers.\n",
        "\n",
        "Why Are They Important?\n",
        "\n",
        "* Visualizing Learning: Feature maps help us understand what the model is learning at each layer.\n",
        "* Debugging and Interpretability: By visualizing feature maps, we can see if the model is focusing on the right parts of the input.\n",
        "* Transfer Learning: Pre-trained models use feature maps learned from large datasets (like ImageNet) as a starting point for new tasks."
      ],
      "metadata": {
        "id": "-o8Oqa5CHmOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_base_model = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)\n",
        ")\n",
        "\n",
        "vgg_base_model.summary()"
      ],
      "metadata": {
        "id": "RgiyIjc_Q6yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = [layer.output for layer in vgg_base_model.layers]\n",
        "feature_map_model = Model(\n",
        "    inputs=vgg_base_model.input,\n",
        "    outputs=feature_maps)\n",
        "\n",
        "\n",
        "feature_map_model.summary()"
      ],
      "metadata": {
        "id": "b7412mTc9GOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Activation Mapping (CAM)\n",
        "\n",
        "Class Activation Mapping (CAM) is a technique used in Convolutional Neural Networks (CNNs) to highlight the important regions in an image that contribute to a specific class prediction. CAM provides interpretability by generating a heatmap that overlays on the original image, showing which areas influenced the model’s decision."
      ],
      "metadata": {
        "id": "3uGvHCMdCOJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grand-cam\n",
        "\n",
        "Grand-CAM (Gradient-weighted Class Activation Mapping) is a technique used in Convolutional Neural Networks (CNNs), to visualize which regions of an image contribute the most to a model's decision. It is an extension of CAM (Class Activation Mapping) but uses gradients to improve flexibility.\n",
        "\n",
        "How Grand-CAM Works\n",
        "* Feature Map Extraction – The convolutional layers of a CNN extract feature maps from the input image.\n",
        "* Gradient Computation – The gradients of the target class score (final prediction) are computed concerning the feature maps. All gradients will be set to 0 except the target class.\n",
        "* Weight Calculation – The gradients are averaged spatially to obtain importance weights for each feature map.\n",
        "* Heatmap Generation – A weighted sum of the feature maps is taken, followed by a ReLU activation to remove negative values.\n",
        "* Overlay on Original Image – The heatmap is resized and overlaid on the original image to highlight important regions."
      ],
      "metadata": {
        "id": "rU1AbQjuWDSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain a model\n",
        "\n",
        "# Initialize the EfficientNetB5 model as the backbone, without the top layers and using ImageNet weights.\n",
        "backbone_model = tf.keras.applications.efficientnet.EfficientNetB5(\n",
        "    include_top = False,  # Exclude the top classification layer\n",
        "    weights = \"imagenet\",  # Use pretrained ImageNet weights\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),  # Define input image shape\n",
        ")\n",
        "\n",
        "# Set the backbone model to non-trainable (freeze its layers).\n",
        "backbone_model.trainable = False\n",
        "\n",
        "# Get the output of the backbone model.\n",
        "x = backbone_model.output\n",
        "\n",
        "# Add a global average pooling layer to reduce spatial dimensions of feature maps.\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully connected (Dense) layer with ReLU activation and a specified number of neurons (N_DENSE_1).\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\")(x)\n",
        "\n",
        "# Add batch normalization to stabilize training by normalizing activations.\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Add dropout regularization to prevent overfitting.\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Add another fully connected (Dense) layer with ReLU activation and a specified number of neurons (N_DENSE_2).\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\")(x)\n",
        "\n",
        "# Add batch normalization again to stabilize training.\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Add another dropout layer for further regularization.\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Add the final output layer with softmax activation for multi-class classification.\n",
        "pretrained_output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")(x)\n",
        "\n",
        "# Create the complete model by defining inputs and outputs, using the EfficientNetB5 backbone.\n",
        "pretrained_model = Model(inputs=backbone_model.inputs, outputs=pretrained_output, name=\"EfficientNetB5_pretrained\")\n",
        "\n",
        "# Display a summary of the model architecture.\n",
        "pretrained_model.summary()\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "# Early stopping callback: stop training if the validation loss doesn't improve for 3 epochs.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# Reduce learning rate on plateau callback: reduce learning rate by a factor of 0.1 if validation loss doesn't improve.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Model checkpoint callback: save the best model based on validation accuracy.\n",
        "model_checkpoint = ModelCheckpoint('pretrained_eff_b5.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "# List of callbacks to be used during training.\n",
        "callbacks = [early_stopping, reduce_lr, model_checkpoint]\n",
        "\n",
        "# Fit the model using the training data with the CutMix data augmentation technique.\n",
        "history = pretrained_model.fit(\n",
        "    mixed_dataset.map(cutmix),  # Apply CutMix augmentation to the training dataset\n",
        "    validation_data=validation_dataset,  # Use a validation dataset for evaluation\n",
        "    epochs=10,  # Train the model for 10 epochs\n",
        "    verbose=1,  # Show training progress\n",
        "    callbacks=callbacks  # Use the defined callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "iG7VD_yCDuTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model weights from a saved file\n",
        "pretrained_model.load_weights('/content/pretrained_eff_b5.keras')\n",
        "\n",
        "# Read and preprocess a test image\n",
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/angry/101071.jpg_rotation_1.jpg\")  # Read image from the specified path\n",
        "test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))  # Resize the image to match the model input size\n",
        "\n",
        "# Convert the image to a TensorFlow constant and add an extra dimension to represent batch size\n",
        "im = tf.constant(test_image, dtype = tf.float32)\n",
        "img_array = tf.expand_dims(im, axis = 0)  # Add batch dimension (shape: 1, height, width, channels)\n",
        "\n",
        "# Print the shape of the image array (should be [1, height, width, channels])\n",
        "print(img_array.shape)\n",
        "\n",
        "# Make a prediction using the model and map the predicted class index to its name\n",
        "prediction = CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(pretrained_model(img_array), axis = -1).numpy()[0]]\n",
        "print(prediction)\n",
        "\n",
        "# Get the last convolutional layer's output\n",
        "last_conv_layer_name = \"top_activation\"  # Define the last convolutional layer name\n",
        "last_conv_layer = pretrained_model.get_layer(last_conv_layer_name)  # Retrieve the layer by its name\n",
        "\n",
        "# Create a new model that outputs the activations of the last convolutional layer\n",
        "last_conv_layer_model = tf.keras.Model(inputs = pretrained_model.inputs, outputs = last_conv_layer.output, name = \"final_conv_layer\")\n",
        "last_conv_layer_model.summary()  # Print the summary of the model (structure of the last conv layer)\n",
        "\n",
        "# Define the classifier layers to use for final predictions\n",
        "classifier_layer_names = [\n",
        "    \"global_average_pooling2d\",  # Layer name for global average pooling\n",
        "    \"dense\",  # Fully connected layer 1\n",
        "    \"dense_1\",  # Fully connected layer 2\n",
        "    \"dense_2\"  # Fully connected layer 3\n",
        "]\n",
        "\n",
        "# Define the input shape for the classifier model based on the last conv layer's output shape\n",
        "classifier_input = Input(shape= last_conv_layer.output.shape[1:])\n",
        "x = classifier_input  # Set input to the model\n",
        "\n",
        "# Apply the layers from the classifier to the input\n",
        "for layer in classifier_layer_names:\n",
        "    x = pretrained_model.get_layer(layer)(x)  # Pass the input through each layer\n",
        "\n",
        "# Create a classifier model from the input and output\n",
        "classifier_model = Model(classifier_input, x)\n",
        "\n",
        "# Use GradientTape to record the gradient calculation for backpropagation\n",
        "with tf.GradientTape() as tape:\n",
        "    # Get the output of the last conv layer\n",
        "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "\n",
        "    # Get the classifier model's prediction\n",
        "    prediction = classifier_model(last_conv_layer_output)\n",
        "\n",
        "    # Get the index of the class with the highest predicted probability\n",
        "    top_pred_index = tf.argmax(prediction[0])\n",
        "    top_class_channel = prediction[:, top_pred_index]  # Extract the output of the top predicted class\n",
        "\n",
        "# Calculate the gradients of the top predicted class with respect to the output of the last convolutional layer\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "# Pool the gradients by averaging them across spatial dimensions (height and width)\n",
        "pool_grads = tf.reduce_mean(grads, axis=(0,1,2)).numpy()\n",
        "\n",
        "# Convert the output from the last conv layer to a numpy array for further processing\n",
        "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "\n",
        "# Apply the pooled gradients to the output of the last convolutional layer (weighted sum)\n",
        "for i in range(len(pool_grads)):\n",
        "    last_conv_layer_output[:,:,i] *= pool_grads[i]\n",
        "\n",
        "# Create a heatmap by summing across the channel axis (depth) of the feature map\n",
        "heatmap = np.sum(last_conv_layer_output, axis=-1)\n",
        "heatmap = tf.nn.relu(heatmap)  # Apply ReLU activation to ensure all values are positive\n",
        "\n",
        "# Convert the heatmap to a numpy array\n",
        "heatmap_np = heatmap.numpy()\n",
        "\n",
        "# Normalize the heatmap values to the range [0, 1]\n",
        "heatmap_np = heatmap_np / np.max(heatmap_np)\n",
        "\n",
        "# Resize the heatmap to match the original image size\n",
        "resized_heatmap = cv2.resize(heatmap_np, (img_array.shape[2], img_array.shape[1]))\n",
        "\n",
        "# Apply a color map (jet) to the heatmap to make it visually interpretable\n",
        "colored_heatmap = cv2.applyColorMap(np.uint8(255 * resized_heatmap), cv2.COLORMAP_JET)\n",
        "\n",
        "# Convert the original image from TensorFlow format to a uint8 format (between 0 and 255)\n",
        "original_img = np.uint8(img_array[0] * 255)\n",
        "\n",
        "# If the original image is grayscale, convert it to BGR format for display\n",
        "if original_img.shape[-1] == 1:\n",
        "    original_img = cv2.cvtColor(original_img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "# Blend the original image with the colored heatmap (using alpha blending)\n",
        "alpha = 0.5  # Define the transparency of the overlay\n",
        "overlay = cv2.addWeighted(original_img, 1 - alpha, colored_heatmap, alpha, 0)\n",
        "\n",
        "# Display the final overlay image with the heatmap on top\n",
        "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))  # Convert from BGR to RGB for proper display\n",
        "plt.axis(\"off\")  # Remove axis from the plot\n",
        "plt.show()  # Show the image\n"
      ],
      "metadata": {
        "id": "gcnlEWXcfWod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}