{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOClVeQWM7r9t3R3Cv9/7GW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeabwang/Human-Emotions-Detection/blob/main/Note_on_state_of_art_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.cm as cm\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from tensorflow.keras.regularizers  import L2, L1\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Example, Features, Feature\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "SPWL-PrnmXMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Sequential API\n",
        "\n",
        "The Sequential API is the simplest way to create a model in TensorFlow using Keras. It allows you to stack layers sequentially, meaning each layer has exactly one input tensor and one output tensor. This is suitable for most feedforward neural networks where the model is a straight line of layers."
      ],
      "metadata": {
        "id": "-QdO5HO_ea7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 3,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "\n",
        "lenet_model = tf.keras.Sequential(\n",
        "    [\n",
        "    InputLayer(shape = (None, None, 3), ),\n",
        "    # Accepts images of any height and width (None, None) with 3 color channels (RGB).\n",
        "    # This makes the model flexible to different image sizes.\n",
        "\n",
        "    resize_rescale_layers,\n",
        "    # Resizes images to a fixed shape (e.g., 32x32 or 64x64) for the model.\n",
        "    # Rescales pixel values (e.g., from [0, 255] to [0, 1]) for normalization.\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"] , kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"] , padding='valid',\n",
        "          activation = 'relu',kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = CONFIGURATION[\"POOL_SIZE\"], strides= CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "    Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"] ),\n",
        "\n",
        "    # Conv2D Layer:\n",
        "    # filters: Number of filters for feature extraction.\n",
        "    # kernel_size: Size of the convolution window (e.g., 3x3).\n",
        "    # strides: Step size for sliding the kernel over the input.\n",
        "    # padding='valid': No padding, reducing output size.\n",
        "    # activation='relu': Introduces non-linearity.\n",
        "    # kernel_regularizer=L2(...): Applies L2 regularization to reduce overfitting.\n",
        "\n",
        "    # BatchNormalization:\n",
        "    # Normalizes the output of the Conv2D layer, speeding up training and providing regularization.\n",
        "    # MaxPool2D:\n",
        "\n",
        "    # pool_size: Size of the pooling window.\n",
        "    # strides: How far the pooling window moves each step.\n",
        "    # Reduces spatial dimensions while preserving key features.\n",
        "\n",
        "    # Dropout:\n",
        "    # rate: Fraction of neurons to drop (e.g., 0.5 drops 50%).\n",
        "    # Prevents overfitting by promoting generalization.\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"]*2 + 4, kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid',\n",
        "          activation = 'relu', kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = CONFIGURATION[\"POOL_SIZE\"], strides= CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "\n",
        "    Flatten(),\n",
        "    # Converts the 2D feature maps into a 1D vector, preparing it for fully connected layers.\n",
        "\n",
        "    Dense( CONFIGURATION[\"N_DENSE_1\"], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "\n",
        "    Dense( CONFIGURATION['N_DENSE_2'], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense Layers:\n",
        "\n",
        "    # N_DENSE_1 and N_DENSE_2: Number of neurons in each fully connected layer.\n",
        "    # activation='relu': Non-linearity for complex pattern learning.\n",
        "    # kernel_regularizer=L2(...): Regularization to reduce overfitting.\n",
        "    # BatchNormalization and Dropout:\n",
        "\n",
        "    # Consistent use of Batch Norm for faster convergence.\n",
        "    # Dropout helps in generalization by randomly turning off neurons.\n",
        "\n",
        "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\"),\n",
        "    # Output Layer:\n",
        "    # NUM_CLASSES: Number of classes for classification (e.g., 10 for CIFAR-10).\n",
        "    # activation='softmax': Converts outputs to probabilities for each class.\n",
        "\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "Hjx7leiPfsvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Functional API\n",
        "\n",
        "Unlike the Sequential API, where layers are stacked one after another, the Functional API lets you define the computation graph as a directed acyclic graph (DAG).\n",
        "You explicitly define the flow of data between layers, making it suitable for complex neural network architectures.\n",
        "\n",
        "###When to Use the Functional API:\n",
        "When you need multiple inputs or outputs.\n",
        "When you want to use shared layers.\n",
        "When building architectures with skip connections or residual blocks.\n",
        "When you need more flexibility and control over the model design."
      ],
      "metadata": {
        "id": "4QxPcF2ThprY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.regularizers import L2\n",
        "\n",
        "# Configuration Dictionary\n",
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,  # Target image size after resizing\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 3,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "\n",
        "# Resize and Rescale Layer\n",
        "# - Resizes input images to (IM_SIZE, IM_SIZE)\n",
        "# - Rescales pixel values from [0, 255] to [0, 1]\n",
        "resize_rescale_layer = tf.keras.Sequential([\n",
        "    layers.Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# ========================\n",
        "#    Feature Extractor\n",
        "# ========================\n",
        "\n",
        "# Input Layer\n",
        "# - Accepts images of any height and width with 3 color channels\n",
        "future_input = Input(shape=(None, None, 3))\n",
        "\n",
        "# Resize and Rescale Layer\n",
        "# - Ensures all images are of the same size and normalized\n",
        "x = resize_rescale_layer(future_input)\n",
        "\n",
        "# Convolutional Layer 1\n",
        "# - Extracts low-level features like edges and textures\n",
        "# - Uses L2 regularization to prevent overfitting\n",
        "x = Conv2D(filters=CONFIGURATION[\"N_FILTERS\"],\n",
        "           kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides=CONFIGURATION[\"N_STRIDES\"],\n",
        "           padding='valid',\n",
        "           activation='relu',\n",
        "           kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "\n",
        "# Batch Normalization\n",
        "# - Normalizes the output of Conv layer\n",
        "# - Accelerates training and provides regularization\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Max Pooling Layer\n",
        "# - Reduces spatial dimensions\n",
        "# - Keeps only the most prominent features\n",
        "x = MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"],\n",
        "                 strides=CONFIGURATION[\"N_STRIDES\"]*2)(x)\n",
        "\n",
        "# Convolutional Layer 2\n",
        "# - Extracts more complex patterns by increasing filters\n",
        "x = Conv2D(filters=CONFIGURATION[\"N_FILTERS\"]*2 + 4,\n",
        "           kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
        "           strides=CONFIGURATION[\"N_STRIDES\"],\n",
        "           padding='valid',\n",
        "           activation='relu',\n",
        "           kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "\n",
        "# Batch Normalization\n",
        "# - Normalizes output of the second Conv layer\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Max Pooling Layer\n",
        "# - Further reduces spatial dimensions\n",
        "x = MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"],\n",
        "                 strides=CONFIGURATION[\"N_STRIDES\"]*2)(x)\n",
        "\n",
        "# Flatten Layer\n",
        "# - Converts 2D feature maps to 1D feature vector\n",
        "future_output = Flatten()(x)\n",
        "\n",
        "# Create Feature Extractor Model\n",
        "future_model = Model(inputs=future_input, outputs=future_output, name=\"Feature_Extractor\")\n",
        "\n",
        "# ========================\n",
        "#   Classification Model\n",
        "# ========================\n",
        "\n",
        "# Input Layer for Classification Model\n",
        "class_input = Input(shape=(None, None, 3))\n",
        "\n",
        "# Use Feature Extractor\n",
        "# - Reuses the feature extraction part of the model\n",
        "x = future_model(class_input)\n",
        "\n",
        "# Fully Connected Layer 1\n",
        "# - Learns complex patterns and combinations of features\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_1\"],\n",
        "          activation='relu',\n",
        "          kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "\n",
        "# Batch Normalization\n",
        "# - Speeds up training and stabilizes learning\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Dropout Layer\n",
        "# - Randomly sets a fraction of inputs to 0\n",
        "# - Prevents overfitting by adding noise\n",
        "x = Dropout(rate=CONFIGURATION[\"DROPOUT_RATE\"])(x)\n",
        "\n",
        "# Output Layer\n",
        "# - Dense layer with softmax activation for multi-class classification\n",
        "class_output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax')(x)\n",
        "\n",
        "# Create Classification Model\n",
        "class_model = Model(inputs=class_input, outputs=class_output, name=\"Classification_Model\")\n",
        "\n",
        "# Model Summary\n",
        "class_model.summary()"
      ],
      "metadata": {
        "id": "nUVQ9lYdlHH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction to Model Subclassing in TensorFlow\n",
        "\n",
        "Model subclassing in TensorFlow is a flexible way to build neural networks by directly inheriting from tf.keras.Model. This approach allows you to fully customize the forward pass (call() method) and define complex architectures that aren't easily implemented using the Sequential or Functional APIs.\n",
        "\n"
      ],
      "metadata": {
        "id": "I3zEvaN6eNPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,  # Target image size after resizing\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 3,\n",
        "    \"DROPOUT_RATE\": 0.2,\n",
        "    \"REGULARIZATION_RATE\": 1e-4,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "\n",
        "# Preprocessing Layer: Resizes images and rescales pixel values to [0, 1]\n",
        "resize_rescale_layer = tf.keras.Sequential([\n",
        "    layers.Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# Feature Extractor Class\n",
        "class LeNetModelFutureExtractor(Layer):\n",
        "    def __init__(self, config):\n",
        "        super(LeNetModelFutureExtractor, self).__init__()\n",
        "\n",
        "        # Store configuration for reuse\n",
        "        self.config = config\n",
        "\n",
        "        # Preprocessing Layer\n",
        "        self.resize_rescale = resize_rescale_layer\n",
        "\n",
        "        # First Convolutional Block\n",
        "        self.conv1 = layers.Conv2D(\n",
        "            filters=config[\"N_FILTERS\"],           # Number of filters\n",
        "            kernel_size=config[\"KERNEL_SIZE\"],     # Size of the convolution kernel\n",
        "            strides=config[\"N_STRIDES\"],           # Stride length\n",
        "            padding='valid',                       # No padding, reduces output size\n",
        "            activation='relu',                     # Activation function for non-linearity\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"]) # L2 regularization\n",
        "        )\n",
        "        self.bn1 = layers.BatchNormalization()    # Normalizes activations to stabilize learning\n",
        "        self.pool1 = layers.MaxPooling2D(\n",
        "            pool_size=config[\"POOL_SIZE\"],          # Downsamples feature map size\n",
        "            strides=config[\"N_STRIDES\"]*2           # Stride for pooling\n",
        "        )\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        self.conv2 = layers.Conv2D(\n",
        "            filters=config[\"N_FILTERS\"]*2 + 4,     # Increasing number of filters\n",
        "            kernel_size=config[\"KERNEL_SIZE\"],\n",
        "            strides=config[\"N_STRIDES\"],\n",
        "            padding='valid',\n",
        "            activation='relu',\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"])\n",
        "        )\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.pool2 = layers.MaxPooling2D(\n",
        "            pool_size=config[\"POOL_SIZE\"],\n",
        "            strides=config[\"N_STRIDES\"]*2\n",
        "        )\n",
        "\n",
        "        # Flatten Layer\n",
        "        self.flatten = layers.Flatten()            # Converts 2D feature maps to 1D vector\n",
        "\n",
        "    def call(self, x):\n",
        "        # Forward pass through the feature extractor\n",
        "        x = self.resize_rescale(x)                 # Resize and Rescale Input\n",
        "        x = self.conv1(x)                          # First Convolutional Layer\n",
        "        x = self.bn1(x)                            # Batch Normalization\n",
        "        x = self.pool1(x)                          # Max Pooling\n",
        "        x = self.conv2(x)                          # Second Convolutional Layer\n",
        "        x = self.bn2(x)                            # Batch Normalization\n",
        "        x = self.pool2(x)                          # Max Pooling\n",
        "        x = self.flatten(x)                        # Flatten for Dense Layers\n",
        "        return x\n",
        "\n",
        "# Classification Model Class\n",
        "class LeNetClassification(Model):\n",
        "    def __init__(self, config):\n",
        "        super(LeNetClassification, self).__init__()\n",
        "\n",
        "        # Feature Extractor (Reusable Component)\n",
        "        self.feature_extractor = LeNetModelFutureExtractor(config)\n",
        "\n",
        "        # Fully Connected Layer 1\n",
        "        self.fc1 = layers.Dense(\n",
        "            config[\"N_DENSE_1\"],                   # Number of neurons in the layer\n",
        "            activation='relu',                     # Activation function\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"]) # L2 regularization\n",
        "        )\n",
        "        self.bn3 = layers.BatchNormalization()     # Normalizes activations\n",
        "        self.dropout1 = layers.Dropout(             # Dropout for regularization\n",
        "            rate=config[\"DROPOUT_RATE\"]\n",
        "        )\n",
        "\n",
        "        # Fully Connected Layer 2\n",
        "        self.fc2 = layers.Dense(\n",
        "            config[\"N_DENSE_2\"],\n",
        "            activation='relu',\n",
        "            kernel_regularizer=L2(config[\"REGULARIZATION_RATE\"])\n",
        "        )\n",
        "        self.bn4 = layers.BatchNormalization()\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = layers.Dense(\n",
        "            config[\"NUM_CLASSES\"],                 # Number of output classes\n",
        "            activation='softmax'                   # Softmax for multi-class classification\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Forward pass through the classification model\n",
        "        x = self.feature_extractor(x)              # Extract Features\n",
        "        x = self.fc1(x)                            # Fully Connected Layer 1\n",
        "        x = self.bn3(x)                            # Batch Normalization\n",
        "        x = self.dropout1(x)                       # Dropout\n",
        "        x = self.fc2(x)                            # Fully Connected Layer 2\n",
        "        x = self.bn4(x)                            # Batch Normalization\n",
        "        x = self.output_layer(x)                   # Output Layer\n",
        "        return x\n",
        "\n",
        "# Instantiate the Classification Model\n",
        "leNet_class_model = LeNetClassification(CONFIGURATION)\n",
        "\n",
        "# Build Model with Input Shape to initialize weights\n",
        "leNet_class_model.build((None, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3))\n",
        "\n",
        "# Display Model Summary\n",
        "leNet_class_model.summary()"
      ],
      "metadata": {
        "id": "vJQFimXNrQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frrl2MIc0Zag"
      },
      "outputs": [],
      "source": [
        "#AlexNet was a breakthrough in deep learning, winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 and pioneering modern CNN architectures.\n",
        "\n",
        "# Architecture: 8 layers (5 convolutional + 3 fully connected).\n",
        "\n",
        "# Input Size: 227 × 227 × 3 (RGB images).\n",
        "\n",
        "# Convolutional Layers:\n",
        "#### Conv1: 96 filters, 11×11 kernel, stride 4, ReLU.\n",
        "#### Conv2: 256 filters, 5×5 kernel, stride 1, ReLU.\n",
        "#### Conv3: 384 filters, 3×3 kernel, stride 1, ReLU.\n",
        "#### Conv4: 384 filters, 3×3 kernel, stride 1, ReLU.\n",
        "#### Conv5: 256 filters, 3×3 kernel, stride 1, ReLU.\n",
        "\n",
        "# Max Pooling: After Conv1, Conv2, and Conv5 (3×3 kernel, stride 2).\n",
        "\n",
        "# Fully Connected Layers:\n",
        "#### FC6: 4096 neurons, ReLU.\n",
        "#### FC7: 4096 neurons, ReLU.\n",
        "#### FC8 (Output): 1000 neurons (ImageNet classes), Softmax.\n",
        "\n",
        "# Activation Function: ReLU (introduced to speed up training).\n",
        "# Normalization: Local Response Normalization (LRN) after Conv1 and Conv2.\n",
        "# Regularization: Dropout (0.5) in FC6 and FC7.\n",
        "# Optimization: Stochastic Gradient Descent (SGD) with momentum (0.9).\n",
        "# Batch Size: 128.\n",
        "# Weight Initialization: Gaussian distribution.\n",
        "# Data Augmentation: Cropping, flipping, and color jittering.\n",
        "# Training Dataset: ImageNet (1.2 million images, 1000 classes).\n",
        "# Parallel Training: Two GPUs used to split model layers for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## VGG Model\n",
        "# Key Features:\n",
        "# Deep Network: 16 (VGG16) or 19 (VGG19) layers.\n",
        "# Uniform Kernel Size: Only 3×3 convolution layers to maintain consistency.\n",
        "# Increased Depth: More layers compared to AlexNet for hierarchical feature learning.\n",
        "# Regularization: Dropout (0.5) in fully connected layers.\n",
        "# Optimization: SGD with momentum (0.9), batch size = 256.\n",
        "# Weight Initialization: Pretrained on ImageNet, useful for transfer learning.\n",
        "# Data Augmentation: Cropping, flipping, and color jittering\n",
        "# VGG16 and VGG19 are the most common variants. #the main difference here is the number of convulational neurons used vgg16 used 13 convulational neurons and the vgg 19 uses the 16 convulational neurons\n",
        "# Stacked small convolutional filters (3×3 kernel, stride 1, padding 1) for deeper representations.\n",
        "# Uses 2×2 max pooling (stride 2) after every block for downsampling.\n",
        "\n",
        "\n",
        "# Layers - Vgg16\n",
        "# Input Size: 224 × 224 × 3 (RGB images).\n",
        "\n",
        "# Conv Layers:\n",
        "#### Block 1: 2 × (64 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 2: 2 × (128 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 3: 3 × (256 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 4: 3 × (512 filters, 3×3, ReLU) → Max Pooling\n",
        "#### Block 5: 3 × (512 filters, 3×3, ReLU) → Max Pooling\n",
        "\n",
        "# Fully Connected Layers:\n",
        "#### FC6: 4096 neurons, ReLU\n",
        "#### FC7: 4096 neurons, ReLU\n",
        "#### FC8 (Output): 1000 neurons (Softmax for classification)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wz7UjWm053uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RESNET MODEL\n",
        "\n",
        "# ResNet introduced residual learning to address the vanishing gradient problem, allowing for extremely deep networks.\n",
        "\n",
        "# Key Features:\n",
        "#### Deep Architecture: Can scale up to ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152.\n",
        "#### Residual Connections (Skip Connections):\n",
        "####### Instead of directly learning H(x), it learns F(x) = H(x) - x, making optimization easier.\n",
        "#### Helps gradients flow smoothly during backpropagation.\n",
        "#### Batch Normalization: Used after every convolution to stabilize training.\n",
        "#### ReLU Activation: Applied after each convolutional layer.\n",
        "\n",
        "# ResNet-18 Layer-by-Layer Breakdown\n",
        "# Here is the detailed layer-wise breakdown for ResNet-18:\n",
        "\n",
        "# Conv1 (Initial Convolutional Layer):\n",
        "\n",
        "# Operation: 7×7 Convolution, 64 filters, stride 2\n",
        "# Output Size: 112 × 112 × 64\n",
        "# MaxPool:\n",
        "\n",
        "# Operation: 3×3 Max Pooling, stride 2\n",
        "# Output Size: 56 × 56 × 64\n",
        "# Conv2_x (Residual Block 1 and 2):\n",
        "\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 64 filters)\n",
        "# Output Size: 56 × 56 × 64\n",
        "# Conv3_x (Residual Block 3 and 4):\n",
        "\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 128 filters), stride 2\n",
        "# Output Size: 28 × 28 × 128\n",
        "# Conv4_x (Residual Block 5 and 6):\n",
        "\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 256 filters), stride 2\n",
        "# Output Size: 14 × 14 × 256\n",
        "# Conv5_x (Residual Block 7 and 8):\n",
        "\n",
        "# Operation: 2 × Basic Residual Blocks (each with 2x 3×3 convolutions, 512 filters), stride 2\n",
        "# Output Size: 7 × 7 × 512\n",
        "# AvgPool (Global Average Pooling):\n",
        "\n",
        "# Operation: Global Average Pooling\n",
        "# Output Size: 1 × 1 × 512\n",
        "# Fully Connected (FC):\n",
        "\n",
        "# Operation: Fully Connected layer (512 → 1000 classes)\n",
        "# Output Size: 1 × 1 × 1000 (classification result)\n",
        "\n",
        "\n",
        "## So we can see ResNet as a collection of shallow layers with a condition of skipping layers which their cumulative is zero.\n",
        "## Firstly this will help the model avoid vanishing gradient.\n",
        "## Seconly it performs well since it acts like a collection of various shallow layers which the model choose its path based on the conditions.\n",
        "\n"
      ],
      "metadata": {
        "id": "bgyAriga_yfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Covariate Shift and Batch Normalization\n",
        "\n",
        "# # Covariate Shift\n",
        "# # Covariate Shift refers to a situation where the distribution of the input data changes between training and testing phases, but the conditional distribution of the output given the input remains the same. In simpler terms, it happens when the model is trained on data from one distribution, but when deployed, it encounters data from a different distribution, which can hurt model performance.\n",
        "\n",
        "# # Batch Normalization (BatchNorm)\n",
        "# # Batch Normalization is a technique introduced to address internal covariate shift during the training of deep neural networks. It normalizes the activations of each layer by scaling and shifting them, ensuring that the distribution of inputs to each layer remains stable throughout training.\n",
        "\n",
        "# In 2D Global Average Pooling, the pooling operation averages over all spatial dimensions (height and width) for each feature map (channel) of the input.\n",
        "# Instead of using traditional pooling methods like max pooling (which extracts the maximum value), global average pooling computes the average value of each feature map over its entire spatial area.\n",
        "\n",
        "# In tasks where the position of the pixels in our datasample don't matter we are good to use the global avg pooling, but if the position matters its not recommended to use that."
      ],
      "metadata": {
        "id": "8bw9XadsTgwL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}